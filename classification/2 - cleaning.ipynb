{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata as ud\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import string\n",
    "import sys, traceback, logging\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words as w\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "# Init Conf\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stem(text):\n",
    "#     tokens = word_tokenize(text)\n",
    "#     result = ' '.join([str(ps.stem(token)) for token in tokens if token not in stops])  \n",
    "# #     result = ' '.join([token for token in tokens if token not in stops])\n",
    "#     sentences = sent_tokenize(result)\n",
    "#     return (result, sentences)\n",
    "    \n",
    "def extract_code(body):\n",
    "    #closing open code sections\n",
    "    if(len(triple_backtick.findall(body))//2 == 1):\n",
    "        body += \"```\"\n",
    "        \n",
    "    code = \"\"\n",
    "    for code_regex in code_regexes:\n",
    "        code_sections = code_regex.findall(body)\n",
    "        code += \" \"\n",
    "        code += \" \".join(code_sections)\n",
    "        for code_section in code_sections:\n",
    "            body = body.replace(code_section, ' codereplaced ')\n",
    "    return(code, body)\n",
    "\n",
    "def extract_url(body):\n",
    "    urls = url_regex.findall(body)\n",
    "    body_urls = \"\"\n",
    "    body_urls += \" \".join(urls)\n",
    "    for url in urls:\n",
    "        body = body.replace(url, ' urlreplaced ')\n",
    "    return(body_urls, body)\n",
    "\n",
    "def extract_email(body):\n",
    "    emails = email_regex.findall(body)\n",
    "    body_emails = \"\"\n",
    "    body_emails += \" \".join(emails)\n",
    "    for email in emails:\n",
    "        body = body.replace(email, ' emailreplaced ')\n",
    "    return(body_emails, body)\n",
    "\n",
    "def camel_case_split(str): \n",
    "    words = [[str[0]]] \n",
    "    for c in str[1:]: \n",
    "        if words[-1][-1].islower() and c.isupper(): \n",
    "            words.append(list(c)) \n",
    "        else: \n",
    "            words[-1].append(c) \n",
    "    return ' '.join([''.join(word) for word in words])\n",
    "\n",
    "def deal_with_camel_cases(text):\n",
    "    #it was tokenize, changed to split in order to keep @codes together\n",
    "    #previous one changed it to @ code, I'am to I 'am\n",
    "    tokens = text.split()\n",
    "    result = ' '.join([camel_case_split(token) for token in tokens])  \n",
    "    return result\n",
    "\n",
    "def relace_words_list(text, wordlist):    \n",
    "    _ = []\n",
    "    for word in text.split():\n",
    "        if word.lower() in wordlist:\n",
    "            _.append(wordlist[word.lower()])\n",
    "        else:\n",
    "            _.append(word.lower())\n",
    "    return \" \".join(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '!\"$%&\\()*,/:;<=>[\\\\]^`{|}~+'#omitted ' for contractions, excluded # @ . ? -\n",
    "replace_string = ' '*len(punctuations)\n",
    "num_of_alpha_char_regex = re.compile(r'[a-zA-Z_]')\n",
    "#url_regex = r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))'''\n",
    "url_regex = re.compile(r'\\(?https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+')\n",
    "email_regex = re.compile( r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "triple_backtick = re.compile(r'```')\n",
    "code_regexes = [re.compile(r'```.*?```',re.DOTALL), re.compile(r'``.*?``',re.DOTALL), re.compile(r'`.*?`',re.DOTALL)]\n",
    "word_regex = re.compile(r'\\w+')\n",
    "ascii_regex = re.compile(r'[^\\x00-\\x7f]')\n",
    "date_regex = re.compile(r'\\d{4}-\\d{2}-\\d{2}')\n",
    "time_regex = re.compile(r'\\d{2}:\\d{2}:\\d{2}')\n",
    "issue_regex = re.compile(r'#[0-9]+')\n",
    "#start with a / or start with a word and continue having /. also covers ../, ././, ...\n",
    "path_regex = re.compile(r'([a-zA-Z0-9._-]+)?(?:\\.{2})?(?:\\/\\.{2})*((\\/){1,2}[a-zA-Z0-9._-]+)+')\n",
    "function_regex = re.compile(r'[a-zA-Z][a-zA-Z0-9_.]*\\([a-zA-Z0-9_, ]*\\)')\n",
    "literal_nonascii_char_regex = re.compile(r'\\\\x[0-9]{2,}')#literally \\x00 \\xff...\n",
    "#sinleQoute and dash at the start of sententence\n",
    "qoute_starter_regex = re.compile(r\"^'\")\n",
    "dash_starter_regex = re.compile(r'^-')\n",
    "dash_qoute_starter_space_regex = re.compile(r\"(^| )(-|')+\")\n",
    "space_dash_qoute_end_regex = re.compile(r\"(-|')+($| )\")\n",
    "consecutive_dashs_qoute_regex = re.compile(r\"[\\-|']{2,}\")\n",
    "num_regex = re.compile(r'[0-9]+(th)?(nd)?(rd)? ')\n",
    "#numbers = '0123456789'\n",
    "numbers = re.compile(r'\\b([A-Za-z]*)\\d+\\b|\\b\\d+([A-Za-z]*)\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"'ll\": \"will\",\n",
    "    \"n't\": \"not\",\n",
    "    \"'ve\": \"have\",\n",
    "    \"'m\": \"am\",\n",
    "    \"'s\": \"is\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"dont\": \"does not\",\n",
    "    \"cant\": \"does not\",    \n",
    "    \"wont\": \"will not\",\n",
    "    \"let's\": \"let us\",#####-----------------------\n",
    "    \"ain't\": \"are not\",#am not / \n",
    "    \"aren't\": \"are not\",# / am not\n",
    "    \"can't\": \"can not\",\n",
    "    \"can't've\": \"can not have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",# / he would\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",#he shall / \n",
    "    \"he'll've\": \"he will have\",#he shall have / \n",
    "    \"he's\": \"he is\",#he has / \n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",#how has / \n",
    "    \"i'd\": \"I would\",#I had / \n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I will\",#I shall / \n",
    "    \"i'll've\": \"I will have\",#I shall have / \n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",# / it would\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",#it shall / \n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",# / it has, it shall have / \n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",# / she would\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",#she shall / \n",
    "    \"she'll've\": \"she will have\",#she shall have / \n",
    "    \"she's\": \"she is\",#she has / \n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",#so as / \n",
    "    \"that'd\": \"that had\",#that would / \n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",#that has / \n",
    "    \"there'd\": \"there would\",#there had / \n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",#there has / \n",
    "    \"they'd\": \"they would\",#they had / \n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",#they shall / \n",
    "    \"they'll've\": \"they will have\",#they shall have / \n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",#we had / \n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",#what shall / \n",
    "    \"what'll've\": \"what will have\",#what shall have / \n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",#what has / \n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",#when has / \n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",#where has / \n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",#who shall / \n",
    "    \"who'll've\": \"who will have\",#who shall have / \n",
    "    \"who's\": \"who is\",#who has / \n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",#why has / \n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",#you had / \n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",#you shall / \n",
    "    \"you'll've\": \"you will have\",#you shall have / \n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "abrv_top50 = {   \"2moro\":\"tomorrow\",\"2nite\":\"tonight\", \n",
    "                 \"btw\":\"moreover\",#\"by the way\",#moreover                   \n",
    "                 \"gr8\":\"great\", \n",
    "                 \"imho\":\"opinion\",                \n",
    "                 \"j/k\":\"kidding\", \n",
    "                 \"lmao\":\"laughing\",    \n",
    "                 \"lol\":\"laughing\",                 \n",
    "                 \"nub\":\"new person\", \n",
    "                 \"noob\":\"new person\", \n",
    "                 \"newbe\":\"new person\", \n",
    "                 \"omg\":\"disbelief\", #\"Oh my god\", \n",
    "                 \"pov\":\"point\",#\"point of view\", \n",
    "                 \"plz\":\"please\",\n",
    "                 \"thx\":\"thanks\",  \n",
    "                 \"thks\":\"thanks\", \n",
    "                 \"rtm\":\"read manual\", \n",
    "                 \"rtfm\":\"read manual\", \n",
    "                 \"wtf\":\"annoyment\",#\"what the fuck\" \n",
    "                 \"i.e.\": \"meaning\",\n",
    "                 \".i.e.\": \"meaning\",\n",
    "                 \"ie\": \"meaning\",\n",
    "                 \"e.g.\": \"example\",\n",
    "                 \".e.g.\": \"example\",\n",
    "                 \"eg\": \"example\"\n",
    "                }\n",
    "                 #\"rt\":\"RealTime\", \"irl\":\"In Real Life\", \"fwiw\":\"for what it is worth\",\"mhoty\":\"My Hat is Off To You\", \n",
    "                 #\"TLC\":\"Tender Loving Care\",#\"BRB\":\"Be Right Back\",\n",
    "                 #\"tmi\":\"Too MuchI nformation\",#\"B4N\":\"Bye For Now\",                  \n",
    "                 #\"BCNU\":\"Be Seeing You\", \n",
    "                 #\"BFF\":\"Best Friends Forever\", \n",
    "                 #\"CYA\":\"Cover Your Ass\", \n",
    "                 #\"DBEYR\":\"Don't Believe Everything You Read\",      \n",
    "                 #\"DILLIGAS\":\"Do I Look Like I Give A Shit\", \n",
    "                 #\"FUD\":\"Fear, Uncertainty and Disinformation\", \n",
    "                 #\"TTYL\":\"Talk To You Later\", \n",
    "                 #\"TYVM\":\"Thank You Very Much\",#\"ILY\":\"I Love You\", \n",
    "                 #\"VBG\":\"Very Big Grin\",#\"ISO\":\"In Search Of\", \n",
    "                 #\"WEG\":\"Wicked Evil Grin\",\n",
    "                 #\"WYWH\":\"Wish You Were Here\",#\"LYLAS\":\"Love You Like A Sister\", \n",
    "                 #\"XOXO\":\"Hugs and Kisses\"\n",
    "                 #\"SITD\":\"Still In The Dark\",#\"NIMBY\":\"Not In My BackYard\", \n",
    "                 #\"NP\":\"No Problem\", \n",
    "                 #\"SOL\": \"Sooner Or Later\", \n",
    "                 #\"STBY\":\"Sucks To Be You\",\n",
    "                 #\"SWAK\":\"Sealed With A Kiss\",#\"OIC\":\"Oh, I See\",\n",
    "                 #\"TFH\":\"Thread From Hell\",#\"OT\":\"Off Topic\",\n",
    "                 #\"TX\":\"Thanks\",#\"RBTL\":\"Read Between The Lines\", \n",
    "                 #\"ROTFLMAO\":\"Rolling On The Floor Laughing My Ass Off\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added plural form as well\n",
    "abrv_CS_common = { \n",
    "    \"alloc\": \"allocate\",\n",
    "    \"auto\": \"automatic\",\n",
    "    \"arg\": \"argument\",\n",
    "    \"args\": \"argument\",\n",
    "    \"attr\": \"attribute\",\n",
    "    \"app\": \"application\",\n",
    "    \"apps\": \"application\",\n",
    "    \"async\": \"asynchronous\",\n",
    "    \"bg\": \"background\",\n",
    "    \"bin\": \"binary\",\n",
    "    \"bool\": \"boolean\",\n",
    "    \"btn\": \"button\",\n",
    "    \"calc\": \"calculate\",\n",
    "    'clr': \"clear\",\n",
    "    'cmd': \"command\",\n",
    "    \"cmp\": \"compare\",\n",
    "    'cnt': \"counter\",\n",
    "    \"cont\": \"continue\",\n",
    "    \"config\": \"configuration\",\n",
    "    \"cfg\": \"configuration\",\n",
    "    'col': \"column\",\n",
    "    \"ctrl\": \"control\",\n",
    "    \"cpy\": \"copy\",\n",
    "    \"db\": 'database',\n",
    "    \"docs\": 'documents',\n",
    "    \"doc\": 'documents',\n",
    "    \"dev\": \"device\",\n",
    "    \"disp\": \"display\",\n",
    "    \"env\": \"environment\",\n",
    "    \"err\": \"error\",\n",
    "    \"func\": \"function\",\n",
    "    \"foo\": \"function\",\n",
    "    \"img\": \"image\",\n",
    "    \"info\": \"information\",\n",
    "    #\"infos\": \"information\",\n",
    "    \"init\": \"initialize\",\n",
    "    \"lib\": \"library\",\n",
    "    \"gen\": \"generate\",\n",
    "    \"math\": \"mathematics\",\n",
    "    \"mcu\": \"microcontroller\",\n",
    "    \"mid\": \"middle\",\n",
    "    \"misc\": \"miscellaneous\",\n",
    "    \"mng\": \"manager\",\n",
    "    \"msg\": \"message\",\n",
    "    \"notif\": \"notification\",\n",
    "    \"num\": \"number\",\n",
    "    \"obj\": \"object\",\n",
    "    \"oo\": \"object-oriented\",\n",
    "    \"param\": \"parameter\",\n",
    "    \"params\": \"parameter\",\n",
    "    \"pic\": \"picture\",\n",
    "    \"prev\": \"previous\",\n",
    "    \"pkg\": \"package\",\n",
    "    \"ptr\": \"pointer\",\n",
    "    \"px\": \"pixel\",\n",
    "    \"src\": \"source\",\n",
    "    \"spec\": \"specification\",\n",
    "    \"seq\": \"sequence\",\n",
    "    \"sync\": \"synchronous\",\n",
    "    \"temp\": \"temporary\",\n",
    "    \"tgl\": \"toggle\",\n",
    "    \"tmr\": \"timer\",\n",
    "    \"txt\": \"text\",\n",
    "    \"v\": \"version\",\n",
    "    \"ver\": \"version\",\n",
    "    \"vb\": \"visual basic\",\n",
    "    \"vm\": \"virtual machine\",\n",
    "    \"val\": \"value\",\n",
    "    \"var\": \"variable\",\n",
    "    \"vert\": \"vertical\",\n",
    "    \"win\": \"windows\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace months and date which are in the regex format of date\n",
    "dates_list = {'january': 'datereplaced','february': 'datereplaced','march': 'datereplaced',\n",
    "              'april': 'datereplaced','june': 'datereplaced','july': 'datereplaced','august': 'datereplaced',\n",
    "              'september': 'datereplaced','october': 'datereplaced','november': 'datereplaced','december': 'datereplaced',\n",
    "                        'tuesday':'dayreplaced', 'wednesday':'dayreplaced', 'thursday':'dayreplaced',\n",
    "              'friday':'dayreplaced', 'saturday':'day', 'sunday':'dayreplaced', 'monday':'dayreplaced',\n",
    "              'afternoon':'timereplaced', 'tonight':'timereplaced', 'night':'timereplaced', 'morning': 'timereplaced',\n",
    "              'tomorrow': 'timereplaced',\"o'clock\": 'timereplaced'}\n",
    "\n",
    "digits_list = {'one': 'digitreplaced','two': 'digitreplaced','three': 'digitreplaced','four': 'digitreplaced',\n",
    "              'five': 'digitreplaced','six': 'digitreplaced','seven': 'digitreplaced','eight': 'digitreplaced',\n",
    "              'nine': 'digitreplaced','ten': 'digitreplaced','eleven': 'digitreplaced','twenty': 'digitreplaced',\n",
    "              'hundred': 'digitreplaced','thousand': 'digitreplaced','million': 'digitreplaced','billion': 'digitreplaced'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omitting not from stop words #hack #change later\n",
    "nltk_stopwords = ['i',\n",
    " 'me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\n",
    " \"you'll\",\"you'd\",'your','yours','yourself','yourselves',\n",
    " 'he','him','his','himself',\n",
    " 'she',\"she's\",'her','hers','herself',\n",
    " 'it',\"it's\",'its','itself',\n",
    " 'they','them','their','theirs','themselves',\n",
    " 'what','which','who','whom',\n",
    " 'this','that',\"that'll\",'these','those',\n",
    " 'am','is','are','was','were',\n",
    " 'be','been','being',\n",
    " 'have','has','had','having',\n",
    " 'do','does','did','doing',\n",
    " 'a','an','the','and','but','if','or','of',\n",
    " 'because','as','until','while',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " #'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " \"don't\",\n",
    " 'should',\n",
    " \"should've\",\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " \"aren't\",\n",
    " 'couldn',\n",
    " \"couldn't\",\n",
    " 'didn',\n",
    " \"didn't\",\n",
    " 'doesn',\n",
    " \"doesn't\",\n",
    " 'hadn',\n",
    " \"hadn't\",\n",
    " 'hasn',\n",
    " \"hasn't\",\n",
    " 'haven',\n",
    " \"haven't\",\n",
    " 'isn',\n",
    " \"isn't\",\n",
    " 'ma',\n",
    " 'mightn',\n",
    " \"mightn't\",\n",
    "# 'mustn',\n",
    "# \"mustn't\",\n",
    " 'needn',\n",
    " \"needn't\",\n",
    " 'shan',\n",
    " \"shan't\",\n",
    " 'shouldn',\n",
    " \"shouldn't\",\n",
    " 'wasn',\n",
    " \"wasn't\",\n",
    " 'weren',\n",
    " \"weren't\",\n",
    " 'won',\n",
    " \"won't\",\n",
    " 'wouldn',\n",
    " \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "my_stopwords = ['use', 'get', 'would', 'etc', 'example', 'also', 'could', 'please', 'thank', 'thanks','seem', \n",
    "                   'instead', 'hi', 'think', 'something', 'may', 'without', 'still', 'sure', 'however', \n",
    "                  'since', 'mean', 'already', 'done', 'well', 'another', 'might', 'via', \n",
    " 'maybee', 'us', 'always', 'let', 'hello', 'given', 'per', 'either', 'got', 'due', 'within', \n",
    " 'shown', 'sometime','sometimes', 'ok','become', 'otherwise', 'else', 'other', 'therefore', 'anymore', 'although', \n",
    " 'often', 'today', 'upon','anyway', 'sorry', 'along', 'foo', 'together', 'across', 'whenever', \n",
    " 'somewherer', 'hey', 'somehow', 'unless', 'behind', 'saw', 'almost', 'whatever', 'welcome', 'everyone', \n",
    " 'went','earlier','accordingly', 'shall', 'prior', 'ago', 'came','ever', 'cool', 'mostly', 'obvious', 'obviously', \n",
    " 'clearly', 'onto', 'anywhere', 'neither', 'hence', '``', '`',  'oh', 'oops','somewhat',\n",
    "'.', 'x', \"'\", 'c', 'e', 'b', 'v', 'f', 'r', 'j', '-', 'h', 'z', 'l', \n",
    " 'p', 'n', 'k', 'w', 'u', 'q', 'bb', 'xx', 'ff', 'fff', 'god', \"'ve\", \"'m\",\"'s\",'ie', 'th', 'nd' 'faq', 'db']\n",
    "\n",
    "\n",
    " #'ie', 'th', 'nd' 'faq', 'db''tech'\n",
    " #'doesnt', 'cant', 'dont', \"n't\" \n",
    "# '#','@', '?', \n",
    "\n",
    "\n",
    "#obligations\n",
    "#'must', \n",
    "extra_stop_words_list = my_stopwords + nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(871878, 32)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/english_issues.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bug        418522\n",
       "feature    363321\n",
       "other       90035\n",
       "Name: label_cat, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(747057, 32)\n"
     ]
    }
   ],
   "source": [
    "#omit empty records\n",
    "#dropping issues without title or body\n",
    "\n",
    "df = df[~df['title'].isna()]\n",
    "df = df[~df['body'].isna()]\n",
    "df = df[df['title']!='']\n",
    "df = df[df['body']!='']\n",
    "df = df[~df['title'].isnull()]\n",
    "df = df[~df['body'].isnull()]\n",
    "\n",
    "#set the types\n",
    "df.title = df.title.astype(str)\n",
    "df.body = df.body.astype(str)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_words_num'] = df['title'].apply(lambda x:len(x.split()))\n",
    "df['body_words_num'] = df['body'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy to start processing\n",
    "df['title_processed'] = df['title']\n",
    "df['title_processed'] = df['title_processed'].astype(str)\n",
    "df['body_processed'] = df['body']\n",
    "df['body_processed'] = df['body_processed'].astype(str)\n",
    "\n",
    "#apply changes to title and body to make it appropriate for detecting language+word distributations\n",
    "print('Converting camel cases...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:deal_with_camel_cases(x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:deal_with_camel_cases(x))\n",
    "\n",
    "#to lower\n",
    "print('Converting to lower case...')\n",
    "df['title_processed'] = df['title_processed'].str.lower()\n",
    "df['body_processed'] = df['body_processed'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove issues with speical titles\n",
    "df = df[~df.title_processed.str.contains(\"not an issue\", na=False)]\n",
    "df = df[~df.title_processed.str.contains(\"not a issue\", na=False)]\n",
    "df = df[~df.title_processed.str.contains(\"its not me\", na=False)]\n",
    "df = df[~df.title_processed.str.contains(\"it's not me\", na=False)]\n",
    "df = df[~df.title_processed.str.startswith(\"oh my god\", na=False)]\n",
    "df = df[~df.title_processed.str.startswith(\"omg\", na=False)]\n",
    "# issues = issues[~issues.title.str.startswith(\"test\", na=False)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove words less than 3 char\n",
    "#it should not be here, bcuz it omits ** or ``` for code...\n",
    "# print('Removing words with less than 3 characters...')\n",
    "# df['raw_title'] = df['raw_title'].str.findall('\\b\\w{1,3}\\b').str.join(' ')\n",
    "# df['raw_body'] = df['raw_body'].str.findall('\\b\\w{1,3}\\b').str.join(' ')\n",
    "\n",
    "#drop issues with title or body length less than 3        \n",
    "df = df[~(df['title_processed'].str.len()<3)]\n",
    "df = df[~(df['body_processed'].str.len()<3)]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace @code/@date/@time/@url/@path/@email before omitting numbers/special characters/puntuations\n",
    "\n",
    "#omit code snippets from title, if any.\n",
    "#omit codes before markdown, since beautifulsoup omits '''\n",
    "# print('Replacing code snippets...')\n",
    "# for regex in code_regexes:\n",
    "#     df['title_processed'] = df['title_processed'].apply(lambda x:regex.sub(\" codereplaced \",x))\n",
    "\n",
    "temp = df['title_processed'].apply(lambda x:extract_code(x))\n",
    "code, title = zip(*temp)\n",
    "df['title_processed'] = list(title)\n",
    "\n",
    "#extract code from body\n",
    "temp = df['body_processed'].apply(lambda x:extract_code(x))\n",
    "code, body = zip(*temp)\n",
    "df['code'] = list(code)\n",
    "df['body_processed'] = list(body)\n",
    "\n",
    "#replacing functions\n",
    "print('Replacing functions...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:function_regex.sub(\" functionreplaced \",x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:function_regex.sub(\" functionreplaced \",x))\n",
    "\n",
    "#replacing urls and emails; extracting them from body\n",
    "print('Replacing emails and urls...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:url_regex.sub(\" urlreplaced \",x))\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:email_regex.sub(\" emailreplaced \",x))\n",
    "#from body\n",
    "temp = df['body_processed'].apply(lambda x:extract_url(x))\n",
    "url, body = zip(*temp)\n",
    "df['url'] = list(url)\n",
    "df['body_processed'] = list(body)\n",
    "temp = df['body_processed'].apply(lambda x:extract_email(x))\n",
    "email, body = zip(*temp)\n",
    "df['email'] = list(email)\n",
    "df['body_processed'] = list(body)\n",
    "\n",
    "#replace date and time\n",
    "print('Replacing dates and times...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:date_regex.sub(\" datereplaced \",x))\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:time_regex.sub(\" timereplaced \",x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:date_regex.sub(\" datereplaced \",x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:time_regex.sub(\" timereplaced \",x))\n",
    "\n",
    "#replace paths/directories\n",
    "print('Replacing paths...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:path_regex.sub(\" pathreplaced \",x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:path_regex.sub(\" pathreplaced \",x))\n",
    "\n",
    "#replace isse numbers as #anynumber\n",
    "print('Replacing issue numbers...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:issue_regex.sub(\" issuereplaced \",x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:issue_regex.sub(\" issuereplaced \",x))\n",
    "\n",
    "#dropping issues without title or body\n",
    "print('Clean up...')\n",
    "df = df[~df['title_processed'].isna()]\n",
    "df = df[~df['body_processed'].isna()]\n",
    "df = df[df['title_processed']!='']\n",
    "df = df[df['body_processed']!='']\n",
    "df = df[~df['title_processed'].isnull()]\n",
    "df = df[~df['body_processed'].isnull()]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Remove Markdown Language such as **, ##, ...\n",
    "# #Join them with space, otherwise words would attach\n",
    "# #this is not complete, the next one is better\n",
    "# def mark_to_plain(t):\n",
    "#     html = markdown(t)\n",
    "#     return ' '.join(BeautifulSoup(html).findAll(text=True))\n",
    "\n",
    "# print('Removing markdowns in titles...')\n",
    "# df['title_processed'] = df['title_processed'].apply(mark_to_plain)\n",
    "# print('Removing markdowns in bodies...')\n",
    "# df['body_processed'] = df['body_processed'].apply(mark_to_plain)\n",
    "\n",
    "\n",
    "#Remove Markdown Language such as **, ##, ...\n",
    "#Join them with space, otherwise words would attach\n",
    "#this is not complete, the next one is better\n",
    "#I manually added title consecutive # and *\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "    html = markdown(markdown_string)\n",
    "\n",
    "    # remove code snippets\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "    html = re.sub(r'[#]{2,6}', ' ', html)\n",
    "    #html = re.sub(r'[*]+', ' ', html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(text=True))\n",
    "\n",
    "    return text\n",
    "\n",
    "print('Removing markdowns in desc_processed...')\n",
    "df['title_processed'] = df['title_processed'].apply(markdown_to_text)\n",
    "print('Removing markdowns in readme_processed...')\n",
    "df['body_processed'] = df['body_processed'].apply(markdown_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_string = ' '*len(punctuations)\n",
    "#removing punctuations before replacing in the next sections, bcuz sth like (e.g. nextWord) won't be replaced otherwise\n",
    "print('Removing punctuations...')\n",
    "df['title_processed'] = df['title_processed'].str.translate(str.maketrans(punctuations, replace_string))\n",
    "df['body_processed'] = df['body_processed'].str.translate(str.maketrans(punctuations, replace_string))\n",
    "\n",
    "#replacing withespaces with space again\n",
    "print('Replacing white spaces...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping issues which number of alphabets appeared in their title is less than 3 or 50% of title length\n",
    "print('Dropping issues for which number of alphabets (title) is less than 3 or 50% of title length')\n",
    "df['title_alpha_len'] = df['title_processed'].apply(lambda x:len(num_of_alpha_char_regex.findall(x)))\n",
    "df = df[df['title_alpha_len'] >= 3]    \n",
    "df['title_alphabet_ratio'] = df[['title_alpha_len','title_processed']].apply(lambda x:x[0]/len(x[1]), axis=1)\n",
    "\n",
    "#dropping issues which number of alphabets appeared in their body is less than 3 or 50% of body length\n",
    "print('Dropping issues for which number of alphabets (body) is less than 3 or 50% of body length')\n",
    "df['body_alpha_len'] = df['body_processed'].apply(lambda x:len(num_of_alpha_char_regex.findall(x)))\n",
    "df = df[df['body_alpha_len'] >= 3]    \n",
    "df['body_alphabet_ratio'] = df[['body_alpha_len','body_processed']].apply(lambda x:x[0]/len(x[1]), axis=1)\n",
    "\n",
    "#clean up\n",
    "df = df[df['title_alphabet_ratio'] >= 0.5 ]\n",
    "df = df[df['body_alphabet_ratio'] >= 0.5 ]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "#do this before replacing contractions, bcuz some of them are stuck with numers and therefore won't be replaced\n",
    "#bcuz contractions work with words not the whole strings\n",
    "#that is \"e.g.3 something\" won't be rplaced\n",
    "print('Removing numbers...')\n",
    "#this can be done after removing numbers, in the second step only reomve nd, th, ... \n",
    "#but then if sth was a \"th\" to symbole sth else it will be omitted\n",
    "\n",
    "#2nd, 3rd, ...\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(num_regex, ' digitreplaced ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(num_regex, ' digitreplaced ', x))\n",
    "\n",
    "#just replace single numbers (without any letter) or digits right after some letters\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(numbers, r'\\1 digitreplaced \\2', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(numbers, r'\\1 digitreplaced \\2', x))\n",
    "\n",
    "# replace_string = ' '*10\n",
    "# df['desc_processed'] = df['desc_processed'].str.translate(str.maketrans(numbers, replace_string))\n",
    "# df['readme_processed'] = df['readme_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "                                                    \n",
    "# df['desc_processed'] = df['desc_processed'].str.translate(str.maketrans(numbers, replace_string))\n",
    "# df['readme_processed'] = df['readme_processed'].apply(lambda x:\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#omitting non-ascii charachters\n",
    "print('Removing non-ascii charachters...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(ascii_regex, '', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(ascii_regex, '', x))    \n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:ud.normalize('NFD', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:ud.normalize('NFD', x))\n",
    "\n",
    "#replacing withespaces with space again\n",
    "print('Replacing white spaces...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#periodic cleanup\n",
    "# Remove White space\n",
    "print('Removing white spaces...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "\n",
    "#dropping issues without readme or fullname\n",
    "df = df[~df['title_processed'].isna()]\n",
    "df = df[df['title_processed']!='']\n",
    "df = df[~df['title_processed'].isnull()]\n",
    "\n",
    "df = df[~df['body_processed'].isna()]\n",
    "df = df[df['body_processed']!='']\n",
    "df = df[~df['body_processed'].isnull()]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing '?' to separate them from words. replacing dots in words such as \"e.g.\" make a problem \n",
    "#but question marks are ok.\n",
    "print('Replacing question marks...')\n",
    "df['title_processed'] = df['title_processed'].str.replace('?', ' ? ') \n",
    "df['body_processed'] = df['body_processed'].str.replace('?', ' ? ')\n",
    "\n",
    "print('Replacing multiple consecutive dots...')\n",
    "df['title_processed'] = df['title_processed'].str.replace('[\\.]{2,}', ' etc ') \n",
    "df['body_processed'] = df['body_processed'].str.replace('[\\.]{2,}', ' etc ')\n",
    "                                                        \n",
    "#omit \"_\"\n",
    "print('Removing _ sign...')\n",
    "df['title_processed'] = df['title_processed'].str.replace('_', ' ')\n",
    "df['body_processed'] = df['body_processed'].str.replace('_', ' ')\n",
    "\n",
    "print('Removing dash from start or end of a word or between words. Keep - in the middle of words')\n",
    "# df['title_processed'] = df['title_processed'].str.replace(\" ' \", ' ')\n",
    "# df['body_processed'] = df['body_processed'].str.replace(\" ' \", ' ')\n",
    "\n",
    "# df['title_processed'] = df['title_processed'].str.replace(' - ', ' ')\n",
    "# df['body_processed'] = df['body_processed'].str.replace(' - ', ' ')\n",
    "\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(dash_qoute_starter_space_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(dash_qoute_starter_space_regex, ' ', x))\n",
    "\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(space_dash_qoute_end_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(space_dash_qoute_end_regex, ' ', x))\n",
    "\n",
    "print('Removing multiple consecutive dashs or qoutes....')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(consecutive_dashs_qoute_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(consecutive_dashs_qoute_regex, ' ', x)) \n",
    "\n",
    "print('Removing literal non-ascii characters such as \\x00....')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(literal_nonascii_char_regex, '', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(literal_nonascii_char_regex, '', x))  \n",
    "\n",
    "print('Removing white spaces...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Contractions\n",
    "print('Replacing contractions...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:relace_words_list(x, contractions))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:relace_words_list(x, contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace abbreviations\n",
    "print('Replacing abbreviations...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:relace_words_list(x, abrv_top50))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:relace_words_list(x, abrv_top50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace common abbreviations in computer science and IT\n",
    "#Do this before markdown, because that will put an space in e.g., I can't and other simialr things\n",
    "\n",
    "print('Replacing IT abbreviations...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:relace_words_list(x, abrv_CS_common))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:relace_words_list(x, abrv_CS_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace month day time of day bcuz of different formats of date...\n",
    "print('Replacing dates...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:relace_words_list(x, dates_list))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:relace_words_list(x, dates_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace digits\n",
    "print('Replacing digits...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:relace_words_list(x, digits_list))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:relace_words_list(x, digits_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the first sentences before anything\n",
    "print('Keeping the first sentences before removing words with less than 3 chars...')\n",
    "df['title_processed_sentences'] = df['title_processed'].str.split('.') \n",
    "df['body_processed_sentences'] = df['body_processed'].str.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing dots and question marks to separate them from words\n",
    "print('Replacing dots...')\n",
    "df['title_processed'] = df['title_processed'].str.replace('.', ' . ') \n",
    "df['body_processed'] = df['body_processed'].str.replace('.', ' . ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some columns\n",
    "#computing body and title len\n",
    "print('Computing body and title len...')\n",
    "df['body_processed_len'] = df['body_processed'].str.len()\n",
    "df['title_processed_len'] = df['title_processed'].str.len()\n",
    "df['title_processed_words_num'] = df['title_processed'].apply(lambda x:len(x.split()))\n",
    "df['body_processed_words_num'] = df['body_processed'].apply(lambda x:len(x.split()))\n",
    "\n",
    "print('Computing number of different abstract elements...')\n",
    "df['num_of_sharps'] = df['body_processed'].str.count('#')\n",
    "df['num_of_at'] = df['body_processed'].str.count('@')\n",
    "df['num_of_qmark'] = df['body_processed'].str.count('\\?')\n",
    "df['num_of_codesnippets'] = df['title_processed'].str.count('codereplaced')+df['body_processed'].str.count('codereplaced')\n",
    "df['num_of_functions'] = df['title_processed'].str.count('functionreplaced')+df['body_processed'].str.count('functionreplaced')\n",
    "df['num_of_issues'] = df['title_processed'].str.count('issuereplaced')+df['body_processed'].str.count('issuereplaced')\n",
    "df['num_of_paths'] = df['title_processed'].str.count('pathreplaced')+df['body_processed'].str.count('pathreplaced')\n",
    "df['num_of_dates'] = df['title_processed'].str.count('datereplaced')+df['body_processed'].str.count('datereplaced')\n",
    "df['num_of_times'] = df['title_processed'].str.count('timereplaced')+df['body_processed'].str.count('timereplaced')\n",
    "df['num_of_urls'] = df['title_processed'].str.count('urlreplaced')+df['body_processed'].str.count('urlreplaced')\n",
    "df['num_of_emails'] = df['title_processed'].str.count('emailreplaced')+df['body_processed'].str.count('emailreplaced')\n",
    "df['num_of_obligations'] = df['title_processed'].str.count('must')+df['body_processed'].str.count('must')\n",
    "\n",
    "df['has_email'] = df['email'].apply(lambda x: 1 if '@' in x else 0)\n",
    "df['has_code'] = df['num_of_codesnippets'].apply(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#periodic cleanup\n",
    "# Remove White space\n",
    "print('Removing white spaces...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:\" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop issues without title or body\n",
    "print('Removing empty records..')\n",
    "df = df[~df['title_processed'].isna()]\n",
    "df = df[~df['body_processed'].isna()]\n",
    "df = df[~df['title_processed'].isnull()]\n",
    "df = df[~df['body_processed'].isnull()]\n",
    "df = df[df['title_processed']!='']\n",
    "df = df[df['body_processed']!='']\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra stemming\n",
    "print('Removing singleQoute from start or end of a word or between words. Keep - in the middle of words')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(dash_qoute_starter_space_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(dash_qoute_starter_space_regex, ' ', x))\n",
    "\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(space_dash_qoute_end_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(space_dash_qoute_end_regex, ' ', x))\n",
    "\n",
    "def replace_extra_stopwords(text, words_list):   \n",
    "    #first split them then replace\n",
    "    _ = []\n",
    "    for word in text.split():\n",
    "        if word.lower() in words_list:\n",
    "            _.append('')\n",
    "        else:\n",
    "            _.append(word.lower())\n",
    "    return \" \".join(_)\n",
    "\n",
    "print('Replacing extra stop words in title_processed...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:replace_extra_stopwords(x, extra_stop_words_list))\n",
    "print('Replacing extra stop words in body_processed...')\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:replace_extra_stopwords(x, extra_stop_words_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing singleQoute from start or end of a word or between words. Keep - in the middle of words')      \n",
    "#replacing dashs and qoutes from the start\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(dash_qoute_starter_space_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(dash_qoute_starter_space_regex, ' ', x))\n",
    "\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(space_dash_qoute_end_regex, ' ', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(space_dash_qoute_end_regex, ' ', x))\n",
    "\n",
    "#remove # and @ to lower word frequency\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub('#', '', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub('#', '', x))\n",
    "\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub('@', '', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub('@', '', x))\n",
    "\n",
    "print('Replacing dash...')\n",
    "df['title_processed'] = df['title_processed'].str.replace('-', ' ') \n",
    "df['body_processed'] = df['body_processed'].str.replace('-', ' ')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Removing white spaces...')\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:\" \".join(x.split()))\n",
    "\n",
    "print('Removing empty records..')\n",
    "df = df[~df['title_processed'].isna()]\n",
    "df = df[~df['body_processed'].isna()]\n",
    "df = df[df['title_processed']!='']\n",
    "df = df[df['body_processed']!='']\n",
    "df = df[~df['title_processed'].isnull()]\n",
    "df = df[~df['body_processed'].isnull()]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second replace to make sure everything is ok\n",
    "#just replace single numbers (without any letter) or digits right after/before some letters\n",
    "df['title_processed'] = df['title_processed'].apply(lambda x:re.sub(numbers, r'\\1 digitreplaced \\2', x))\n",
    "df['body_processed'] = df['body_processed'].apply(lambda x:re.sub(numbers, r'\\1 digitreplaced \\2', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatiziation\n",
    "stops_list = extra_stop_words_list\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    result = ' '.join([str(lemmatizer.lemmatize(token)) for token in tokens if token not in stops_list])  \n",
    "    sentences = sent_tokenize(result)\n",
    "    return result\n",
    "\n",
    "df['title_proc_lem'] = df['title_processed'].apply(lambda x:lemmatize(str(x)))\n",
    "df['body_proc_lem'] = df['body_processed'].apply(lambda x:lemmatize(str(x)))\n",
    "\n",
    "df['title_lem_len'] = df['title_proc_lem'].apply(lambda x: len(x))\n",
    "df['title_lem_words_num'] = df['title_proc_lem'].apply(lambda x:len(x.split()))\n",
    "df['body_lem_len'] = df['body_proc_lem'].apply(lambda x: len(x))\n",
    "df['body_lem_words_num'] = df['body_proc_lem'].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding number of issue label categories\n",
    "df[\"lower_labels\"] = df.labels.apply(lambda x: str(x).lower())\n",
    "df[\"bug_label\"] = df.lower_labels.apply(lambda x: 1 if \"bug\" in x else 0)\n",
    "df[\"feature_label\"] = df.lower_labels.apply(lambda x: 1 if (\"feature\" in x or \"enhancement\" in x) else 0)\n",
    "df[\"other_label\"] = df.lower_labels.apply(lambda x: 1 if (\"support\" in x or \"docs\" in x or \"documentation\" in x or \"question\" in x) else 0)\n",
    "df[\"mono_label\"] = df[\"bug_label\"] + df[\"feature_label\"] + df[\"other_label\"]\n",
    "df[\"mono_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out issues with more than one label cat\n",
    "df_monolabel = df[df.mono_label == 1]\n",
    "df = []\n",
    "df_monolabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering out inconsistent labels\n",
    "df_monolabel = df_monolabel[((df_monolabel.other_label == 1) & (df_monolabel.label_cat == \"other\")) | ((df_monolabel.bug_label == 1) & (df_monolabel.label_cat == \"bug\")) | ((df_monolabel.feature_label == 1) & (df_monolabel.label_cat == \"feature\"))]\n",
    "df_monolabel.drop(columns=[\"bug_label\", \"feature_label\", \"other_label\", \"mono_label\"], inplace=True)\n",
    "df_monolabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_monolabel.label_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data to train and test\n",
    "y = df_monolabel[\"label_cat\"]\n",
    "train, test = train_test_split(df_monolabel, test_size=0.2, random_state = 42, stratify=y, shuffle=True)\n",
    "df_monolabel = []\n",
    "train[\"test_tag\"] = 0\n",
    "test[\"test_tag\"] = 1\n",
    "df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving clean dataset\n",
    "textonly_columns = [\"number\", \"repository_url\", \"title_processed\", \"body_processed\", \"label_cat\", \"test_tag\"]\n",
    "\n",
    "def save_dataset(df, name):\n",
    "    df.to_csv(f\"data/{name}.csv\", index=False)\n",
    "    df[textonly_columns].to_csv(f'data/{name}_textonly.csv', index=False)\n",
    "    \n",
    "save_dataset(df, \"clean_mono_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.distplot(df_monolabel['title_processed_words_num'], bins =30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.distplot(df_monolabel[df_monolabel.body_processed_words_num<5000][\"body_processed_words_num\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.distplot(df_monolabel[df_monolabel.title_processed_words_num<60].title_processed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,8))\n",
    "# sns.distplot(df_monolabel[df_monolabel.body_processed_len<10000].body_processed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_monolabel[(df_monolabel.title_processed_words_num<60)][df_monolabel.title_processed_len>350].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_monolabel[(df_monolabel.body_processed_words_num<2000)][df_monolabel.body_processed_len>6000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #removing outliers\n",
    "# short_df = df_monolabel[(df_monolabel['title_processed_words_num'] <= 60) &\n",
    "#                         (df_monolabel['body_processed_words_num'] <= 2000) &\n",
    "#                         (df_monolabel['title_processed_len'] <= 350) &                                        \n",
    "#                         (df_monolabel['body_processed_len'] <= 6000)]\n",
    "# save_dataset(short_df,\"short\")\n",
    "# short_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #removing issues that their title or body contains only one word\n",
    "# multi_word_df = df_monolabel[df_monolabel.title_processed_words_num > 1]\n",
    "# multi_word_df = df_monolabel[df_monolabel.body_processed_words_num > 1]\n",
    "# df_monolabel = []\n",
    "# save_dataset(multi_word_df,\"multi\")\n",
    "# multi_word_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #removing outliers AND single word issues\n",
    "# short_multi_df = short_df[short_df.title_processed_words_num > 1]\n",
    "# short_multi_df = short_df[short_df.body_processed_words_num > 1]\n",
    "# multi_word_df = []\n",
    "# short_df = []\n",
    "# save_dataset(short_multi_df,\"short_multi\")\n",
    "# short_multi_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
