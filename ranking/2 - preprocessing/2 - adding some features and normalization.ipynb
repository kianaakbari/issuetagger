{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding fasttext probs and sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import fasttext\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/allrepos_processed.csv\")\n",
    "senti = pd.read_csv(\"data/allrepos_sentiments.csv\")\n",
    "ftprobs = pd.read_csv(\"data/allrepos_ftprobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ft_bug\"] = ftprobs[\"__label__bug\"] \n",
    "df[\"ft_feature\"] = ftprobs[\"__label__feature\"] \n",
    "df[\"ft_other\"] = ftprobs[\"__label__other\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_sentistrenght_p\"] = senti[\"title_sentistrenght\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"title_sentistrenght_n\"] = senti[\"title_sentistrenght\"].apply(lambda x: x.split(',')[1])\n",
    "\n",
    "df[\"body_sentistrenght_p\"] = senti[\"body_sentistrenght\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"body_sentistrenght_n\"] = senti[\"body_sentistrenght\"].apply(lambda x: x.split(',')[1])\n",
    "\n",
    "df[\"title_polarity\"] = senti[\"title_textblob\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"title_subjectivity\"] = senti[\"title_textblob\"].apply(lambda x: x.split(',')[1])\n",
    "\n",
    "df[\"body_polarity\"] = senti[\"body_textblob\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"body_subjectivity\"] = senti[\"body_textblob\"].apply(lambda x: x.split(',')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"positive_title_sentistrenght_n\"] = df[\"title_sentistrenght_n\"].astype(int).abs()\n",
    "df[\"positive_body_sentistrenght_n\"] = df[\"body_sentistrenght_n\"].astype(int).abs()\n",
    "df[\"positive_title_polarity\"] = df[\"title_polarity\"].astype(float) + 1\n",
    "df[\"positive_body_polarity\"] = df[\"body_polarity\"].astype(float) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applynig some other preprocessings and adding some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.closer_login.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_interval(t1, t2):\n",
    "    d1 = datetime.strptime(t1, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    d2 = datetime.strptime(t2, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    delta = d2-d1\n",
    "    return delta.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_time = df.created_at.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"author_account_age\"] = df[\"author_created_at\"].apply(lambda x: round(compute_time_interval(x, base_time)/365))    \n",
    "df[\"closer_account_age\"] = df[\"closer_created_at\"].apply(lambda x: round(compute_time_interval(x, base_time)/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"has_assignee\"] = ~df[\"assignee\"].isna()\n",
    "df[\"num_of_assignees\"] = df[\"assignees\"].apply(lambda x: 0 if pd.isna(x) else len(x.split(\"|\")))\n",
    "df['has_milestone'] = df['milestone'].apply(lambda x: 0 if pd.isna(x) else 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"author_github_cntrb\"] = df[\"author_github_cntrb\"].apply(lambda x: int(str(x).replace(',','')))\n",
    "df[\"closer_github_cntrb\"] = df[\"closer_github_cntrb\"].apply(lambda x: int(str(x).replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"same_author_closer\"] = df.apply(lambda x: x.author_login == x.closer_login, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['numeric_association'] = df['author_association'].apply(lambda x: 0 if x == \"NONE\"  else 1 if x == \"CONTRIBUTOR\" else 2 if x == \"MEMBER\" else 3 if x == \"OWNER\" else 4)\n",
    "# df['numeric_association'] = df['numeric_association'].apply(lambda col:pd.Categorical(col).codes)\n",
    "\n",
    "# dummies = pd.get_dummies(df[\"author_association\"],prefix=\"association\")\n",
    "# df = pd.concat([df,dummies], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"max_prob\"] = df.apply(lambda x: max(x.ft_bug,x.ft_feature,x.ft_other), axis=1)\n",
    "df[\"ft_issue_type\"] = df.apply(lambda x: 2 if x.ft_bug == x.max_prob else 1 if x.ft_feature == x.max_prob else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = df[\"labels\"].astype(str)\n",
    "df[\"num_labels\"] = df[\"labels\"].apply(lambda x: x.count(\"|\"))\n",
    "df[\"lower_labels\"] = df.labels.apply(lambda x: str(x).lower())\n",
    "\n",
    "df.isduplicate = df.lower_labels.apply(lambda x: \"duplicate\" in x)\n",
    "df = df[~df.isduplicate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cats_df = pd.read_csv(\"labels_clusters.csv\")\n",
    "label_cats = list(label_cats_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_cat(x):\n",
    "    issue_labels = x.split(\"|\")  \n",
    "    if(len(set(issue_labels)-set(cat_labels)) == len(set(issue_labels))):\n",
    "        return 0\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_cat in label_cats:\n",
    "    cat_labels = list(label_cats_df[~label_cats_df[label_cat].isna()][label_cat])\n",
    "    df[label_cat] = df[\"lower_labels\"].apply(has_cat)\n",
    "    print(label_cat)\n",
    "    print(df[label_cat].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this section once, then u can comment it\n",
    "\n",
    "repos_df = pd.read_excel(\"p0to5_labels_alllangs.xlsx\")\n",
    "repos_class_map = {}\n",
    "\n",
    "for index, row in repos_df.iterrows():\n",
    "    repo = row[\"repo\"][29:] \n",
    "    repo_class_map = {}\n",
    "    for cat in [\"class1\", \"class2\"]:\n",
    "        labels = row[cat]\n",
    "        for char in labels[1:]:\n",
    "            repo_class_map[\"p\"+char] = cat\n",
    "    repos_class_map[repo] = repo_class_map\n",
    "    \n",
    "with open(\"repos_class_map.json\" , \"w\") as f:\n",
    "    f.write(json.dumps(repos_class_map, indent = 4))\n",
    "    \n",
    "repos = list(repos_class_map.keys())\n",
    "\n",
    "with open(\"2class_repo_addresses.json\", \"w\") as f:\n",
    "    f.write(json.dumps(repos, indent=4))\n",
    "    \n",
    "with open(\"2class_repo_names.json\", \"w\") as f:\n",
    "    f.write(json.dumps([repo.split('/')[1] for repo in repos], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"repos_class_map.json\") as f:\n",
    "    repos_class_map = json.loads(f.read())\n",
    "    \n",
    "df[\"repo_label_2class\"] = df.apply(lambda x: np.nan if x.repo not in repos_class_map else repos_class_map[x.repo][x.actual_label_cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"repo_label_2class\"].isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non english issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.load_model('../classification/data/lid.176.bin')\n",
    "\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"body\"] = df[\"body\"].astype(str)\n",
    "df[\"title_ft\"] = df[\"title\"].apply(lambda x: x.replace('\\n', ' '))\n",
    "df[\"body_ft\"] = df[\"body\"].apply(lambda x: x.replace('\\n', ' '))\n",
    "\n",
    "title_langs = model.predict(list(df[\"title_ft\"]))[0]\n",
    "body_langs = model.predict(list(df[\"body_ft\"]))[0]\n",
    "\n",
    "df[\"title_lang\"] = list(map(lambda x: x[0] , title_langs))\n",
    "df[\"body_lang\"] = list(map(lambda x: x[0] , body_langs))\n",
    "title_langs.clear()\n",
    "body_langs.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"title_lang\"]==\"__label__en\"].shape)\n",
    "print(df[df[\"body_lang\"]==\"__label__en\"].shape)\n",
    "print(df[(df[\"title_lang\"]==\"__label__en\") & (df[\"body_lang\"]==\"__label__en\")].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_lang\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[\"title_lang\"]==\"__label__en\") & (df[\"body_lang\"]==\"__label__en\")]\n",
    "df.drop(columns=[\"title_lang\", \"body_lang\", \"title_ft\", \"body_ft\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "reaction_time_med = {}\n",
    "\n",
    "with open(\"p_repo_addresses.json\") as f:\n",
    "    repo_addresses = json.loads(f.read())\n",
    "    \n",
    "with open(\"p_repo_names.json\") as f:\n",
    "    repos = json.loads(f.read())\n",
    "\n",
    "for i, repo in enumerate(repos):\n",
    "    \n",
    "    repo_df = df[df.repo == repo_addresses[i]]\n",
    "    train, test = train_test_split(repo_df, test_size=0.2, random_state = 42, shuffle=True)\n",
    "    train[\"test_tag\"] = 0\n",
    "    test[\"test_tag\"] = 1\n",
    "    repo_df = pd.concat([train, test], ignore_index=True)\n",
    "    dataframes[repo] = repo_df\n",
    "    reaction_time_med[repo] = repo_df.reaction_time.median()   \n",
    "    print(repo, reaction_time_med[repo])\n",
    "    repo_df.to_csv(f\"data/{repo}.csv\", index=False)\n",
    "\n",
    "repo = 'cross_repo'\n",
    "repo_df = df\n",
    "reaction_time_med[repo] = repo_df.reaction_time.median()  \n",
    "train, test = train_test_split(repo_df, test_size=0.2, random_state = 42, shuffle=True)\n",
    "train[\"test_tag\"] = 0\n",
    "test[\"test_tag\"] = 1\n",
    "repo_df = pd.concat([train, test], ignore_index=True)\n",
    "repo_df.to_csv(f\"data/{repo}.csv\", index=False)\n",
    "dataframes[repo] = repo_df\n",
    "repos += [repo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontext_columns = [    \n",
    "    'num_labels',\n",
    "    'is_pull_request',\n",
    "    'title_len',\n",
    "    'body_len',\n",
    "    'num_comments',\n",
    "    'num_events',\n",
    "    'author_followers',\n",
    "    'closer_followers',\n",
    "    'author_following',\n",
    "    'closer_following',\n",
    "    'author_public_repos',\n",
    "    'closer_public_repos',\n",
    "    'author_public_gists',\n",
    "    'closer_public_gists',\n",
    "    'author_core_team',\n",
    "    'author_has_association',\n",
    "    'author_issue_counts',\n",
    "    'commits_count',\n",
    "    'has_commit',\n",
    "    'cm_developers_number',\n",
    "    'cm_developers_ratio',\n",
    "    'cm_developers_unique',\n",
    "    'cm_authors_unique',\n",
    "    'cm_developers_ratio_unique',\n",
    "    'cm_mean_len',\n",
    "    'time_to_discuss',\n",
    "    'author_github_cntrb',\n",
    "    'closer_github_cntrb',\n",
    "    'author_repo_cntrb',\n",
    "    'closer_repo_cntrb',\n",
    "    'title_words_num',\n",
    "    'body_words_num',   \n",
    "    'title_alpha_len',\n",
    "    'title_alphabet_ratio',\n",
    "    'body_alpha_len',\n",
    "    'body_alphabet_ratio',\n",
    "    'body_processed_len',\n",
    "    'title_processed_len',\n",
    "    'title_processed_words_num',\n",
    "    'body_processed_words_num',\n",
    "    'num_of_sharps',\n",
    "    'num_of_at',\n",
    "    'num_of_qmark',\n",
    "    'num_of_codesnippets',\n",
    "    'num_of_functions',\n",
    "    'num_of_issues',\n",
    "    'num_of_paths',\n",
    "    'num_of_dates',\n",
    "    'num_of_times',\n",
    "    'num_of_urls',\n",
    "    'num_of_emails',\n",
    "    'num_of_obligations',\n",
    "    'has_email',\n",
    "    'has_code',    \n",
    "    'ft_bug',\n",
    "    'ft_feature',\n",
    "    'ft_other',\n",
    "    'max_prob',\n",
    "    'ft_issue_type',\n",
    "    'title_sentistrenght_p',\n",
    "    'body_sentistrenght_p',\n",
    "    'title_subjectivity',\n",
    "    'body_subjectivity',\n",
    "    'positive_body_sentistrenght_n',\n",
    "    'positive_title_sentistrenght_n',\n",
    "    'positive_title_polarity',\n",
    "    'positive_body_polarity',\n",
    "    'author_account_age',\n",
    "    'closer_account_age',\n",
    "    'has_assignee',\n",
    "    'num_of_assignees',\n",
    "    'has_milestone',\n",
    "    'numeric_association'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in repos:\n",
    "    print(repo)\n",
    "    df = dataframes[repo]\n",
    "    train = df[df.test_tag==0]\n",
    "    test = df[df.test_tag==1]\n",
    "    \n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    train[nontext_columns]  = min_max_scaler.fit_transform(train[nontext_columns])\n",
    "    test[nontext_columns]  = min_max_scaler.transform(test[nontext_columns])\n",
    "    df = pd.concat([train, test], ignore_index=True)\n",
    "    \n",
    "    df[\"priority_med\"] = df.reaction_time.apply(lambda x: 2 if x<=reaction_time_med[repo] else 1 if x>reaction_time_med[repo] else 0)\n",
    "    \n",
    "    df.to_csv(f\"data/{repo}_norm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
