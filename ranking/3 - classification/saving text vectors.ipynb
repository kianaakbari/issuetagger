{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from gensim.models import Doc2Vec\n",
    "# from gensim.models import doc2vec\n",
    "# from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import _pickle as cPickle\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cross_label_norm.csv\")\n",
    "\n",
    "for c in [\"title_processed\", \"body_processed\"]:\n",
    "    df[c] = df[c].astype(str)\n",
    "    \n",
    "train = df[df.test_tag == 0]\n",
    "test = df[df.test_tag == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: please create a \"vectors\" directory inside the data directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tfidf_vectors(ngram_range, title_max_features, body_max_features, column_postfix):\n",
    "    \n",
    "    tfidf_vectorizer_title = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=title_max_features)\n",
    "\n",
    "    X_train_title_tfidf = tfidf_vectorizer_title.fit_transform(train[f\"title_{column_postfix}\"])\n",
    "    X_test_title_tfidf = tfidf_vectorizer_title.transform(test[f\"title_{column_postfix}\"])\n",
    "\n",
    "    tfidf_vectorizer_body = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=body_max_features)\n",
    "\n",
    "    X_train_body_tfidf = tfidf_vectorizer_body.fit_transform(train[f\"body_{column_postfix}\"])\n",
    "    X_test_body_tfidf = tfidf_vectorizer_body.transform(test[f\"body_{column_postfix}\"])\n",
    "\n",
    "    column_postfix = column_postfix.replace(\"_\",\"-\")\n",
    "    with open(f\"data/vectors/tfidf_{column_postfix}_train_title\", 'ab') as f:\n",
    "        cPickle.dump(X_train_title_tfidf, f)\n",
    "    with open(f\"data/vectors/tfidf_{column_postfix}_train_body\", 'ab') as f:\n",
    "        cPickle.dump(X_train_body_tfidf, f)\n",
    "    with open(f\"data/vectors/tfidf_{column_postfix}_test_title\", 'ab') as f:\n",
    "        cPickle.dump(X_test_title_tfidf, f)   \n",
    "    with open(f\"data/vectors/tfidf_{column_postfix}_test_body\", 'ab') as f:\n",
    "        cPickle.dump(X_test_body_tfidf, f)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tfidf_vectors(ngram_range = (1,2), title_max_features = 10000, body_max_features = 20000, column_postfix = \"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tfidf finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_sentences(corpus, label_type):\n",
    "#     \"\"\"\n",
    "#     Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "#     We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "#     a dummy index of the post.\n",
    "#     \"\"\"\n",
    "#     labeled = []\n",
    "#     for i, v in enumerate(corpus):\n",
    "#         label = label_type + '_' + str(i)\n",
    "#         labeled.append(doc2vec.TaggedDocument(str(v).split(), [label]))\n",
    "#     return labeled\n",
    "\n",
    "# def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "#     \"\"\"\n",
    "#     Get vectors from trained doc2vec model\n",
    "#     :param doc2vec_model: Trained Doc2Vec model\n",
    "#     :param corpus_size: Size of the data\n",
    "#     :param vectors_size: Size of the embedding vectors\n",
    "#     :param vectors_type: Training or Testing vectors\n",
    "#     :return: list of vectors\n",
    "#     \"\"\"\n",
    "#     vectors = np.zeros((corpus_size, vectors_size))\n",
    "#     for i in range(0, corpus_size):\n",
    "#         prefix = vectors_type + '_' + str(i)\n",
    "#         vectors[i] = model.docvecs[prefix]\n",
    "#     return vectors\n",
    "\n",
    "# def save_d2v_vectors(min_count, title_features_no, body_features_no, column_postfix):\n",
    "#     X_train_title = label_sentences(train[f\"title_{column_postfix}\"], 'Train')\n",
    "#     X_test_title = label_sentences(test[f\"title_{column_postfix}\"], 'Test')\n",
    "\n",
    "#     X_train_body = label_sentences(train[f\"body_{column_postfix}\"], 'Train')\n",
    "#     X_test_body = label_sentences(test[f\"body_{column_postfix}\"], 'Test')\n",
    "\n",
    "#     all_data_title = X_train_title + X_test_title\n",
    "#     all_data_body = X_train_body + X_test_body\n",
    "\n",
    "#     model_dbow_title = Doc2Vec(dm=0, vector_size=title_features_no, negative=5, min_count=min_count, alpha=0.065, min_alpha=0.065)\n",
    "#     model_dbow_title.build_vocab([x for x in tqdm(all_data_title)])\n",
    "#     for epoch in range(30):\n",
    "#         model_dbow_title.train(utils.shuffle([x for x in tqdm(all_data_title)]), total_examples=len(all_data_title), epochs=1)\n",
    "#         model_dbow_title.alpha -= 0.002\n",
    "#         model_dbow_title.min_alpha = model_dbow_title.alpha\n",
    "#     train_title_vectors_dbow = get_vectors(model_dbow_title, len(X_train_title), title_features_no, 'Train')\n",
    "#     test_title_vectors_dbow = get_vectors(model_dbow_title, len(X_test_title), title_features_no, 'Test')\n",
    "\n",
    "#     model_dbow_body = Doc2Vec(dm=0, vector_size=body_features_no, negative=5, min_count=min_count, alpha=0.065, min_alpha=0.065)\n",
    "#     model_dbow_body.build_vocab([x for x in tqdm(all_data_body)])\n",
    "#     for epoch in range(30):\n",
    "#         model_dbow_body.train(utils.shuffle([x for x in tqdm(all_data_body)]), total_examples=len(all_data_body), epochs=1)\n",
    "#         model_dbow_body.alpha -= 0.002\n",
    "#         model_dbow_body.min_alpha = model_dbow_body.alpha\n",
    "#     train_body_vectors_dbow = get_vectors(model_dbow_body, len(X_train_body), body_features_no, 'Train')\n",
    "#     test_body_vectors_dbow = get_vectors(model_dbow_body, len(X_test_body), body_features_no, 'Test')\n",
    "    \n",
    "#     column_postfix = column_postfix.replace(\"_\",\"-\")\n",
    "#     with open(f\"data/vectors/d2v_{title_features_no}-{body_features_no}_{column_postfix}_train_title\", 'ab') as f:\n",
    "#         cPickle.dump(train_title_vectors_dbow, f)\n",
    "#     with open(f\"data/vectors/d2v_{title_features_no}-{body_features_no}_{column_postfix}_train_body\", 'ab') as f:\n",
    "#         cPickle.dump(train_body_vectors_dbow, f)\n",
    "#     with open(f\"data/vectors/d2v_{title_features_no}-{body_features_no}_{column_postfix}_test_title\", 'ab') as f:\n",
    "#         cPickle.dump(test_title_vectors_dbow, f)   \n",
    "#     with open(f\"data/vectors/d2v_{title_features_no}-{body_features_no}_{column_postfix}_test_body\", 'ab') as f:\n",
    "#         cPickle.dump(test_body_vectors_dbow, f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_d2v_vectors(min_count = 2, title_features_no = 500, body_features_no = 500, column_postfix = \"proc_lem\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
