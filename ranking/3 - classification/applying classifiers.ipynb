{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import _pickle as cPickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from itertools import product \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from scipy.stats import uniform\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import _pickle as cPickle\n",
    "from scipy import sparse\n",
    "from tqdm import tqdm\n",
    "from sklearn import utils\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_postfix = {\"tfidf\": \"processed\", \"d2v\": \"500-500_proc-lem\"}\n",
    "\n",
    "def get_features():\n",
    "    ngram_range = (1,2)\n",
    "    title_max_features = 10000\n",
    "    body_max_features = 20000\n",
    "    column_postfix = \"processed\"\n",
    "    \n",
    "    for c in [\"title_processed\", \"body_processed\"]:\n",
    "        df[c] = df[c].astype(str)\n",
    "\n",
    "    train = df[df.test_tag == 0]\n",
    "    test = df[df.test_tag == 1]\n",
    "    \n",
    "    x_train = []\n",
    "    x_test = []\n",
    "        \n",
    "    vectors = {}\n",
    "    train_other_features = train[feature_set]\n",
    "    test_other_features = test[feature_set]\n",
    "\n",
    "    tfidf_vectorizer_title = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=title_max_features)\n",
    "\n",
    "    vectors[\"train_title\"] = tfidf_vectorizer_title.fit_transform(train[f\"title_{column_postfix}\"])\n",
    "    vectors[\"test_title\"] = tfidf_vectorizer_title.transform(test[f\"title_{column_postfix}\"])\n",
    "\n",
    "    tfidf_vectorizer_body = TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        token_pattern=r'\\w{2,}',  #vectorize 2-character words or more\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=body_max_features)\n",
    "\n",
    "    vectors[\"train_body\"] = tfidf_vectorizer_body.fit_transform(train[f\"body_{column_postfix}\"])\n",
    "    vectors[\"test_body\"] = tfidf_vectorizer_body.transform(test[f\"body_{column_postfix}\"])\n",
    "   \n",
    "    x_train = sparse.hstack((vectors[\"train_title\"],vectors[\"train_body\"],train_other_features.astype(float)))\n",
    "    x_test = sparse.hstack((vectors[\"test_title\"],vectors[\"test_body\"],test_other_features.astype(float)))\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "\n",
    "def classify(algorithm, param_mode):\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "\n",
    "    title = f\"{param_mode} {algorithm} + {feature_mode} {file_postfix[feature_mode]}\"    \n",
    "    report = title.strip() + \":\\n\"\n",
    "    \n",
    "    if param_mode == \"default\":\n",
    "        model = classifiers[algorithm][\"clf\"]\n",
    "    elif param_mode == \"specified\":\n",
    "        model = classifiers[algorithm][\"clf_with_params\"]\n",
    "    elif param_mode == \"tuned\":\n",
    "        model = RandomizedSearchCV(estimator=classifiers[algorithm][\"clf\"], param_distributions = classifiers[algorithm][\"random_grid\"], \n",
    "                               n_iter=100, verbose=2, cv=3, random_state=42, n_jobs=n_jobs)\n",
    "        \n",
    "    y_pred = [] \n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)   \n",
    "    report += classification_report(y_test, y_pred)\n",
    "    \n",
    "    if(param_mode == \"tuned\"):\n",
    "        report += \"\\nbestparameters:\\n\" + str(model.best_params_) + '\\n'\n",
    "     \n",
    "    accuracyScore = accuracy_score(y_pred, y_test)\n",
    "    report += \"\\naccuracy score:\" + str(accuracyScore) + '\\n'\n",
    "    \n",
    "    report += \"\\n\\nduration: \" + str(datetime.now() - start_time)\n",
    "    \n",
    "    print(report)   \n",
    "    \n",
    "    with open(f\"results/{repo}_{title}.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"duration: \" + str(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = ['balanced', None]\n",
    "\n",
    "n_jobs = 1\n",
    "random_state = 42\n",
    "\n",
    "rf_random_grid = {'bootstrap': [True, False],\n",
    "                  'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "                  'max_features': ['auto', 'log2', None],\n",
    "                  'min_samples_leaf': [1, 2, 4],\n",
    "                  'min_samples_split': [2, 5, 10],\n",
    "                  'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
    "                  'class_weight': class_weight+[\"balanced_subsample\"]}\n",
    "\n",
    "svc_random_grid = {'C': np.logspace(-3, 2, 6), \n",
    "                   'gamma': ['auto', 'scale'],\n",
    "                   'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                   'class_weight' : class_weight}\n",
    "\n",
    "sgd_random_grid = {\"loss\": [\"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\"],\n",
    "                   \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "                   \"l1_ratio\": 0.2*np.arange(0,6),\n",
    "                   'class_weight' : class_weight}\n",
    "\n",
    "knn_random_grid = {\"leaf_size\" : list(range(1,50)),\n",
    "                   \"n_neighbors\" : list(range(1,35)),\n",
    "                   \"p\": [1,2]}\n",
    "\n",
    "lr_random_grid = {'C' : np.logspace(-3, 2, 6),\n",
    "                  'penalty' : ['l2', 'none'],\n",
    "                  'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "                  'class_weight' : class_weight}\n",
    "\n",
    "classifiers = {\n",
    "    \"mnb\" : {\"clf\" : MultinomialNB()},\n",
    "    \"gnb\" : {\"clf\" : GaussianNB()},\n",
    "    \"lr\" : {\"clf\" : LogisticRegression(n_jobs=n_jobs, random_state=random_state), \"random_grid\" : lr_random_grid, \"clf_with_params\" : LogisticRegression(n_jobs=n_jobs, random_state=random_state, class_weight = 'balanced')},\n",
    "    \"sgd\" : {\"clf\" : SGDClassifier(n_jobs=n_jobs, random_state=random_state), \"random_grid\" : sgd_random_grid, \"clf_with_params\" : SGDClassifier(n_jobs=n_jobs, random_state=random_state)},\n",
    "    \"svc\" : {\"clf\" : SVC(random_state=random_state), \"random_grid\" : svc_random_grid, \"clf_with_params\" : SVC(random_state=random_state, kernel='rbf', gamma='scale', class_weight=None, C=1.0)},    \n",
    "    \"rf\" : {\"clf\" : RandomForestClassifier(n_jobs=n_jobs, random_state=random_state), \"random_grid\" : rf_random_grid, \"clf_with_params\" : RandomForestClassifier(n_jobs=n_jobs, random_state=random_state)},\n",
    "    \"knn\" : {\"clf\" : KNeighborsClassifier(n_jobs=n_jobs), \"random_grid\" : knn_random_grid, \"clf_with_params\" : KNeighborsClassifier(n_jobs=n_jobs)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# issue_features = [       \n",
    "#     'is_pull_request'\n",
    "\n",
    "#     'title_processed_words_num', 'body_processed_words_num', \n",
    "    \n",
    "#     'num_of_codesnippets',\n",
    "#     'num_of_urls',\n",
    "        \n",
    "#     'issue_type',\n",
    "    \n",
    "#     'body_sentistrenght_p',\n",
    "#     'body_subjectivity',\n",
    "#     'positive_body_sentistrenght_n',\n",
    "#     'positive_body_polarity'\n",
    "# ]\n",
    "\n",
    "\n",
    "# user_features = [\n",
    "#     'author_followers', 'author_following', 'author_public_repos', 'author_public_gists', 'author_issue_counts', \n",
    "#     'author_github_cntrb', 'author_repo_cntrb', 'author_account_age', 'numeric_association'\n",
    "# ]\n",
    "\n",
    "labels = pd.read_csv('labels_clusters.csv')\n",
    "\n",
    "label_features = list(labels.columns)\n",
    "\n",
    "selected_features = [         \n",
    "    'ft_issue_type','num_labels',\n",
    "#----------------------------------\n",
    "    'title_processed_words_num','body_processed_words_num','num_of_urls','has_code',\n",
    "#----------------------------------\n",
    "    'has_commit','has_assignee','is_pull_request',\n",
    "#----------------------------------\n",
    "    'same_author_closer','author_followers','author_following','author_public_repos','author_public_gists','author_issue_counts','author_github_cntrb','author_account_age','author_repo_cntrb','numeric_association',\n",
    "#----------------------------------\n",
    "    'closer_followers','closer_following','closer_public_repos','closer_public_gists','closer_repo_cntrb','closer_account_age','closer_github_cntrb',\n",
    "#----------------------------------\n",
    "    'cm_developers_ratio','cm_mean_len',\n",
    "#----------------------------------    \n",
    "    'num_events','num_comments','has_milestone','time_to_discuss',\n",
    "#----------------------------------\n",
    "    'body_sentistrenght_p','positive_body_sentistrenght_n','positive_body_polarity','body_subjectivity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = selected_features + label_features\n",
    "# feature_set = selected_features \n",
    "\n",
    "target_column = \"repo_label_2class\"\n",
    "# target_column = \"repo_label_cat\"\n",
    "\n",
    "feature_mode = \"tfidf\"   \n",
    "\n",
    "param_mode = \"default\"\n",
    "# param_mode = \"specified\"\n",
    "# param_mode = \"tuned\"\n",
    "\n",
    "algorithm_name = \"lr\"\n",
    "\n",
    "smote = True\n",
    "# smote = False\n",
    "\n",
    "norm_data = True\n",
    "# norm_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2class_repo_names.json\") as f:\n",
    "    repo_names = json.loads(f.read())\n",
    "\n",
    "for repo_name in repo_names:\n",
    "    if norm_data:\n",
    "        df = pd.read_csv(f\"data/{repo_name}_norm.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(f\"data/{repo_name}.csv\")\n",
    "        \n",
    "    if repo_name == 'cross_repo':\n",
    "        df = df[df.repo.isin(repo_addresses)]\n",
    "        \n",
    "    y_train = df[df.test_tag == 0][target_column]\n",
    "    y_test = df[df.test_tag == 1][target_column]\n",
    "    x_train, x_test = get_features()\n",
    "\n",
    "    if smote:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        x_train, y_train = sm.fit_resample(x_train, y_train)\n",
    "\n",
    "    print(f'------------------{repo_name}------------------')\n",
    "    classify(algorithm_name, param_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
